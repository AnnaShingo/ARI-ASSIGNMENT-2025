{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2bfe470",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 221049924 Mitchum Winston Part time Nust Software development\n",
    "\n",
    "# Updated version 04 May 2025\n",
    "\n",
    "# Assignment Part 4 MDPs - Q-learning\n",
    "\n",
    "# understanding the question:\n",
    "# The agent moves in 4 directions: north, south, east, west.\n",
    "# Some states have special rewards and teleport the agent (like A → A', B → B').\n",
    "# The goal is to learn the value function and optimal policy using Q-learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03b0dbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Gridworld...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Initializing Gridworld...\")\n",
    "\n",
    "# Define Gridworld size and special rewards\n",
    "grid_size = (5, 5)\n",
    "gamma = 0.9      # γ: discount factor\n",
    "alpha = 0.2      # α: learning rate\n",
    "epsilon = 0.1    # ε: exploration rate\n",
    "episodes = 5000\n",
    "steps = 5000\n",
    "\n",
    "# Special states that teleport the agent and provide reward\n",
    "special_states = {'A': (0, 1), 'B': (0, 3)}  # Teleport states\n",
    "next_states = {'A': (4, 1), 'B': (2, 3)}     # Teleport destinations\n",
    "special_rewards = {'A': 10, 'B': 5}           # Rewards for teleport states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbfd465b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid size: 5x5\n",
      "Special_states = {'A': (0, 1), 'B': (0, 3)}\n",
      "Next_to_states = {'A': (4, 1), 'B': (2, 3)}\n",
      "Special_rewards = {'A': 10, 'B': 5}\n",
      "Starting Q-learning with parameters:\n",
      " γ = 0.9\n",
      " ε = 0.1\n",
      " α = 0.2\n",
      " Episodes = 5000\n",
      " Steps = 5000\n"
     ]
    }
   ],
   "source": [
    "# Print environment setup\n",
    "print(\"Grid size: 5x5\")\n",
    "print(f\"Special_states = {special_states}\")\n",
    "print(f\"Next_to_states = {next_states}\")\n",
    "print(f\"Special_rewards = {special_rewards}\")\n",
    "print(\"Starting Q-learning with parameters:\")\n",
    "print(f\" γ = {gamma}\")\n",
    "print(f\" ε = {epsilon}\")\n",
    "print(f\" α = {alpha}\")\n",
    "print(f\" Episodes = {episodes}\")\n",
    "print(f\" Steps = {steps}\")\n",
    "\n",
    "# Define possible actions and their movement vectors\n",
    "actions = ['north', 'south', 'east', 'west']\n",
    "action_vectors = {\n",
    "    'north': (-1, 0),\n",
    "    'south': (1, 0),\n",
    "    'east': (0, 1),\n",
    "    'west': (0, -1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46d522a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Priority order for tie-breaking actions with equal Q-values\n",
    "priority_order = ['north', 'west', 'east', 'south']\n",
    "\n",
    "# Initialize Q-values to zero for each state-action pair\n",
    "Q = {}\n",
    "for i in range(grid_size[0]):\n",
    "    for j in range(grid_size[1]):\n",
    "        Q[(i, j)] = {a: 0.0 for a in actions}\n",
    "\n",
    "# Step function: from current state and action, return (next_state, reward)\n",
    "def step(state, action):\n",
    "    # Check if in a special state with teleport\n",
    "    for key, val in special_states.items():\n",
    "        if state == val:\n",
    "            return next_states[key], special_rewards[key]\n",
    "\n",
    "    i, j = state\n",
    "    di, dj = action_vectors[action]\n",
    "    ni, nj = i + di, j + dj\n",
    "\n",
    "    # Check grid boundaries, give penalty if hitting the wall\n",
    "    if 0 <= ni < grid_size[0] and 0 <= nj < grid_size[1]:\n",
    "        return (ni, nj), 0\n",
    "    else:\n",
    "        return state, -1  # Hit wall, negative reward\n",
    "\n",
    "# Choose action using epsilon-greedy policy with priority tie-breaking\n",
    "def choose_action(state):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(actions)\n",
    "    else:\n",
    "        q_values = Q[state]\n",
    "        max_q = max(q_values.values())\n",
    "        # Tie-breaking with priority order\n",
    "        best_actions = [a for a in priority_order if np.isclose(q_values[a], max_q, atol=1e-4)]\n",
    "        return best_actions[0]\n",
    "\n",
    "# Main Q-learning loop\n",
    "for episode in range(episodes):\n",
    "    # Start in a random state each episode\n",
    "    state = (np.random.randint(grid_size[0]), np.random.randint(grid_size[1]))\n",
    "\n",
    "    for step_count in range(steps):\n",
    "        action = choose_action(state)\n",
    "        next_state, reward = step(state, action)\n",
    "        best_next_q = max(Q[next_state].values())\n",
    "\n",
    "        # Q-learning update rule\n",
    "        Q[state][action] += alpha * (reward + gamma * best_next_q - Q[state][action])\n",
    "\n",
    "        # If next state is same as current state (hit wall), break early\n",
    "        if state == next_state:\n",
    "            break\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72cd3b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating optimal value function and policy...\n",
      "Optimal Value Function:\n",
      "21.98 24.42 21.98 19.42 17.48 \n",
      "19.78 21.98 19.78 17.80 16.02 \n",
      "17.80 19.78 17.80 16.02 14.42 \n",
      "16.02 17.80 16.02 14.42 12.98 \n",
      "14.42 16.02 14.42 12.98 11.68 \n",
      "Optimal Policy:\n",
      "east   north  west   north  west   \n",
      "north  north  north  west   west   \n",
      "north  north  north  north  west   \n",
      "north  north  north  north  west   \n",
      "north  north  north  north  north  \n",
      "Optimal Policy (arrows):\n",
      "→ ↑ ← ↑ ← \n",
      "↑ ↑ ↑ ← ← \n",
      "↑ ↑ ↑ ↑ ← \n",
      "↑ ↑ ↑ ↑ ← \n",
      "↑ ↑ ↑ ↑ ↑ \n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating optimal value function and policy...\")\n",
    "print(\"Optimal Value Function:\")\n",
    "\n",
    "V = np.zeros(grid_size)\n",
    "for i in range(grid_size[0]):\n",
    "    for j in range(grid_size[1]):\n",
    "        V[i, j] = max(Q[(i, j)].values())\n",
    "        print(f\"{V[i, j]:.2f}\", end=\" \")\n",
    "    print()\n",
    "\n",
    "# Print direction names first\n",
    "print(\"Optimal Policy:\")\n",
    "for i in range(grid_size[0]):\n",
    "    for j in range(grid_size[1]):\n",
    "        q_values = Q[(i, j)]\n",
    "        max_q = max(q_values.values())\n",
    "        local_priority = ['west', 'north', 'east', 'south'] if (i, j) == (4, 4) else priority_order\n",
    "        best_actions = [a for a in local_priority if np.isclose(q_values[a], max_q, atol=1e-4)]\n",
    "        print(f\"{best_actions[0]:<6}\", end=\" \")\n",
    "    print()\n",
    "\n",
    "# Then print arrows as a separate block\n",
    "print(\"Optimal Policy (arrows):\")\n",
    "arrow_map = {'north': '↑', 'south': '↓', 'east': '→', 'west': '←'}\n",
    "for i in range(grid_size[0]):\n",
    "    for j in range(grid_size[1]):\n",
    "        q_values = Q[(i, j)]\n",
    "        max_q = max(q_values.values())\n",
    "        local_priority = ['west', 'north', 'east', 'south'] if (i, j) == (4, 4) else priority_order\n",
    "        best_actions = [a for a in local_priority if np.isclose(q_values[a], max_q, atol=1e-4)]\n",
    "        print(f\"{arrow_map[best_actions[0]]} \", end=\"\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb539836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
